{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9d42ff-64f6-4e0e-b423-c506705f11b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Ignore Tensorflow Warnings\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '2'\n",
    "\n",
    "from tensorflow.keras.layers import AveragePooling2D, UpSampling2D, MaxPooling2D\n",
    "from scipy import interpolate\n",
    "from scipy.io import loadmat\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4310c214-920c-4701-9702-6b09a1e07427",
   "metadata": {},
   "source": [
    "# Split the SWAN matlab file\n",
    "This script allows to convert the SWAN matlab file into a series of csv files for each time step (and sea state variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d879b2b-6e34-4800-a08e-ab7d2816f0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define help function to create a generator that outputs a datetime object for the given time range\n",
    "if time_step_type == \"hrs\":\n",
    "    const = int(24/time_step)\n",
    "    \n",
    "    def daterange(start_date, end_date):\n",
    "        for n in range(const*int((end_date - start_date).days)):\n",
    "            yield start_date + n*timedelta(hours=time_step)\n",
    "\n",
    "else:\n",
    "    const = int(60/time_step * 24)\n",
    "    \n",
    "    def daterange(start_date, end_date):\n",
    "        for n in range(const*int((end_date - start_date).days)):\n",
    "            yield start_date + n*timedelta(minutes=time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badefe36-e880-4e00-a4eb-56d65d5b2cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of file names and constants\n",
    "\n",
    "# Define the type of the time step (hrs, min) and the size of it\n",
    "time_step_type = \"hrs\" # could be min too\n",
    "time_step = 1          # can be something like 20 (for minutes)  \n",
    "\n",
    "# Variable to extract (has to be the name given by SWAN)\n",
    "var = \"Hsig\"\n",
    "var_short = \"Hs\" # How the variable name will appear in the output\n",
    "\n",
    "# Path of the Matlab file to be split\n",
    "fn = \"out/BaskCoast_ZoomBtz.mat\"\n",
    "\n",
    "# Define the data time span\n",
    "start_date = datetime(2018, 1, 1, 0, 0)\n",
    "end_date = datetime(2019, 12, 31, 23, 0)\n",
    "\n",
    "\n",
    "# Path of the output files, can be for reference (HR) or input (LR) files\n",
    "#dirout = \"Data/LR/DS_RAW/Kernel_16/{}/\".format(var_short)\n",
    "dirout = \"Data/HR/{}/\".format(var_short)\n",
    "\n",
    "# Prefix of the output files\n",
    "prefix = \"BaskCoast_{}\".format(var_short.upper())\n",
    "\n",
    "# Check if output folder(s) exists, if not, create it/them\n",
    "if not os.path.isdir(dirout):\n",
    "    os.makedirs(dirout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651eb6e5-eead-4dc7-8226-86eb5a83a261",
   "metadata": {},
   "source": [
    "## Load whole file into RAM\n",
    "This is the easiest way if the whole data set fits in RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ca63df-66aa-4ba1-b656-819a7ab93c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Matlab File\n",
    "data = loadmat(fn)\n",
    "\n",
    "for i, single_date in enumerate(daterange(start_date, end_date)):\n",
    "    # Get the right key for the according day and hour\n",
    "    time = single_date.strftime('%Y%m%d_%H%M00')\n",
    "    file_key = var + \"_\" + time\n",
    "\n",
    "    # Access the array and save it in a file\n",
    "    fn_out = dirout + prefix + \"_{}.csv\".format(i+1)\n",
    "    np.savetxt(fn_out, data[file_key][:,:], delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f682275d-98e7-4331-877b-325d0b2223a2",
   "metadata": {},
   "source": [
    "## Load each time step one by one\n",
    "This is only recommended if the data set fits in RAM. This is not optimized and thus very slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7b518a-ac0d-46e5-8de9-57486c6a852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, single_date in enumerate(daterange(start_date, end_date)):\n",
    "    # Get the right key for according day and hour\n",
    "    time = single_date.strftime('%Y%m%d_%H%M00')\n",
    "    file_key = var + \"_\" + time\n",
    "\n",
    "    data = loadmat(fn, variable_names=(file_key))\n",
    "    # Access the array and save it in a file\n",
    "    fn_out = dirout + prefix + \"_{}.csv\".format(i+1)\n",
    "    np.savetxt(fn_out, data[file_key][:,:], delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa939bc9-1ad9-4712-8a07-576497661d6c",
   "metadata": {},
   "source": [
    "# Upsample low-resolution SWAN data\n",
    "For the training of the neural network the low-resolution data has to be upsampled to have\n",
    "the same grid size as the high-resolution references. This can be done either by a nearest-\n",
    "neighbor scheme or by bicubic interpolation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be16614f-5a9b-4af2-9bf4-ebd94ae119a3",
   "metadata": {},
   "source": [
    "## Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e485ff-e079-4d69-b197-59f7ff768547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of files, grid size and size of Pooling kernel\n",
    "n = 17496\n",
    "pooling_in = \"DS_RAW\"\n",
    "pooling_out = \"DS\"\n",
    "grid = (10, 10) # Of the Low-resolution input\n",
    "kernel = 16\n",
    "var = \"Hs\"\n",
    "\n",
    "# Define file location of HR files \n",
    "fin = \"Data/LR/{}/Kernel_{}/{}/BaskCoast_{}_{{}}.csv\".format(pooling_in, kernel, var, var.upper())\n",
    "\n",
    "# Define directory location of the LR output files\n",
    "dirout = \"Data/LR/{}/Kernel_{}/{}\".format(pooling_out, kernel, var)\n",
    "fout = os.path.join(dirout, \"BaskCoast_{}_{{}}.csv\".format(var.upper()))\n",
    "\n",
    "\n",
    "# Check if output folder(s) exists, if not, create it\n",
    "if not os.path.isdir(dirout):\n",
    "    os.makedirs(dirout)\n",
    "\n",
    "for i in range(1, n+1):\n",
    "    # Load file and convert to tensor\n",
    "    hs = np.loadtxt(fin.format(i), delimiter=',')\n",
    "    hs_t = tf.convert_to_tensor(hs)\n",
    "    hs_t = tf.reshape(hs_t, [1, grid[0], grid[1], 1])\n",
    "    \n",
    "    # Get upsampling object\n",
    "    up_sample = UpSampling2D(size=(kernel, kernel))\n",
    "    \n",
    "    # Upsample to original grid size \n",
    "    hs_t = up_sample(hs_t)\n",
    "    \n",
    "    # Set all negative values to NaN for easier processing during training\n",
    "    hs_t = tf.where(tf.math.less(hs_t, 0), np.nan, hs_t)\n",
    "     \n",
    "    # Save file in specified generic format\n",
    "    np.savetxt(fout.format(i), hs_t[0,:,:,0].numpy(), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb2acbc-811f-4b44-ac47-5e7be6b7979b",
   "metadata": {},
   "source": [
    "## Bicubic Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f3fe2b-afd2-47e6-be33-86384ff5908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of files, grid size and size of Pooling kernel\n",
    "n = 17496\n",
    "pooling_in = \"DS_RAW\"\n",
    "pooling_out = \"DS_INT\"\n",
    "grid = (160, 160) # Of the HR reference\n",
    "kernel = 8\n",
    "grid_LR = tuple(int(xi/kernel) for xi in grid)\n",
    "var = \"Hs\"\n",
    "\n",
    "# Define file location of HR files \n",
    "fin = \"Data/LR/{}/Kernel_{}/{}/BaskCoast_{}_{{}}.csv\".format(pooling_in, kernel, var, var.upper())\n",
    "\n",
    "# Define directory location of the LR output files\n",
    "dirout = \"Data/LR/{}/Kernel_{}/{}\".format(pooling_out, kernel, var)\n",
    "fout = os.path.join(dirout, \"BaskCoast_{}_{{}}.csv\".format(var.upper()))\n",
    "\n",
    "\n",
    "# Check if output folder(s) exists, if not, create it\n",
    "if not os.path.isdir(dirout):\n",
    "    os.makedirs(dirout)\n",
    "\n",
    "# Create HR and LR grids for the interpolation\n",
    "x_HR = np.arange(grid[0])\n",
    "y_HR = np.arange(grid[1])\n",
    "\n",
    "x_LR = np.arange(0, grid[0], kernel)\n",
    "y_LR = np.arange(0, grid[1], kernel)\n",
    "    \n",
    "for i in range(1, n+1):\n",
    "    # Load file and convert to tensor\n",
    "    LR = np.loadtxt(fin.format(i), delimiter=',')\n",
    "    LR = np.nan_to_num(LR)\n",
    "    LR_T = tf.convert_to_tensor(LR)\n",
    "    LR_T = tf.reshape(LR_T, [1, grid_LR[0], grid_LR[1], 1])\n",
    "    \n",
    "    \n",
    "    # Two different interpolation techniques\n",
    "    #f = interpolate.interp2d(x_LR, y_LR, T_LR[0,:,:,0].numpy(), kind='cubic')\n",
    "    f = interpolate.RectBivariateSpline(x_LR, y_LR, LR_T[0,:,:,0].numpy())\n",
    "\n",
    "    # Interpolate to High-Resolution grid\n",
    "    interpol = f(x_HR, y_HR)\n",
    "    interpol[interpol < 0.01] = np.nan\n",
    "     \n",
    "    # Save file in specified generic format\n",
    "    np.savetxt(fout.format(i), interpol, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
