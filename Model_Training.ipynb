{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845f5e04-45e4-4fb8-aad2-a4c7f83a52c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the modified Fukami-model code to train the neural network\n",
    "import os\n",
    "# Ignore Tensorflow Warnings\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '2'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.layers import Input, Dense, Conv2D, Concatenate, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f3eadc-780c-47a2-a284-a4a5c930b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for Summary file\n",
    "summary = \"\"\"Date: {}\n",
    "Kernel: {}\n",
    "Pooling: {}\n",
    "Variable: {}\n",
    "Train_Size: {}\n",
    "Shuffle: {}\n",
    "Patience: {}\n",
    "Batch_Size: {}\n",
    "Time: {}\n",
    "Random_State_Train: {}\n",
    "Random_State_Test: {}\n",
    "sample_start: {}\n",
    "sample_end: {}\n",
    "min_val_loss: {}\n",
    "Eval: {}\n",
    "Augmentation sample size: {}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f55d50-6def-4ed2-8d94-bfdcb9236753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool to add counter if filename already exists\n",
    "def uniquify(paths):\n",
    "    paths_out = []\n",
    "    for path in paths:\n",
    "        fname, ext = os.path.splitext(path)\n",
    "        counter = 1\n",
    "\n",
    "        while os.path.exists(path):\n",
    "            path = \"{}_{}{}\".format(fname, counter, ext)\n",
    "            counter += 1\n",
    "\n",
    "        paths_out.append(path)\n",
    "    return paths_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0dc999-7dc2-45e9-b5ea-0f002df346e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting for Training datasets\n",
    "kernel = 16\n",
    "pooling = \"DS\" # Can be DS for direct simulation, but also Average Pooling, etc\n",
    "var = \"Hs\"\n",
    "\n",
    "# Get the current date\n",
    "today = date.today().strftime(\"%Y%m%d\")\n",
    "\n",
    "# Prepare file locations for reference and input data\n",
    "fname_HR = './Data/HR/{}/BaskCoast_{}_{{}}.csv'.format(var, var.upper())\n",
    "fname_LR = './Data/LR/{}/Kernel_{}/{}/BaskCoast_{}_{{}}.csv'.format(pooling, kernel, var, var.upper())\n",
    "\n",
    "\n",
    "# Beginning and end of the data set serial\n",
    "sample_start = 24\n",
    "sample_end = 8760\n",
    "serial = np.arange(sample_start, sample_end+1, 1)\n",
    "grid = (160,160)\n",
    "dim = 1 # In case of multiple inputs like wave height AND period\n",
    "\n",
    "# Beginning and end of test data serial\n",
    "sample_start_test = 8761\n",
    "sample_end_test = 17496\n",
    "serial_test = np.arange(sample_start_test, sample_end_test+1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc98400-033f-4b8a-8818-f20a9ade5cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning parameters\n",
    "\n",
    "# Training data might be shuffled, however, might affect temporal coherence\n",
    "shuffle = False\n",
    "# Create random integer to set a random state for train_test_split if shuffled\n",
    "random_state_train, random_state_test = np.random.randint(1e6, size=2)\n",
    "\n",
    "train_size = 0.8\n",
    "patience = 30 # For early stopping\n",
    "batch_size = 32 \n",
    "n_augm = False # With or without data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddfd8c6-70f4-453b-8755-500f107e82fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data in to memory\n",
    "\n",
    "# Initialize input and labels arrays respectively\n",
    "X_tot = np.zeros((len(serial), grid[0], grid[1], dim))\n",
    "y_tot = np.zeros((len(serial), grid[0],grid[1], dim))\n",
    "\n",
    "print(\"Loading data...\")\n",
    "\n",
    "# Iterate through the training data set and save reference and input in arrays\n",
    "for i, s in enumerate(serial):\n",
    "\n",
    "    # Load reference data\n",
    "    df = pd.read_csv(fname_HR.format(s), header=None, delim_whitespace=False)\n",
    "    y_tot[i,:,:,0] = df.values\n",
    "\n",
    "    # Load input data\n",
    "    df = pd.read_csv(fname_LR.format(s), header=None, delim_whitespace=False)\n",
    "    X_tot[i,:,:,0] = df.values\n",
    "    \n",
    "\n",
    "# Split into training and validation set\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_tot, y_tot, train_size=train_size, shuffle=shuffle,\n",
    "                                                  random_state=random_state_train)\n",
    "\n",
    "# Discard to free memory\n",
    "del X_tot, y_tot\n",
    "\n",
    "# print(\"Performing data augmentation\")\n",
    "# # Data Augmentation Process\n",
    "# # If number of augmentation samples is not specified, it is as big as original training set\n",
    "# shape = (n_augm, grid[0], grid[1], dim)\n",
    "# if not n_augm:\n",
    "#     shape = X_train.shape\n",
    "#\n",
    "# X_augm = np.empty(shape)\n",
    "# y_augm = np.empty(shape)\n",
    "#\n",
    "# X_train[np.random.choice(X_train.shape[0], n_augm, replace=False)]\n",
    "#\n",
    "# # Arbitrary offset between 0 and 5\n",
    "# offset = np.random.uniform(0, 5, size=(shape[0],1,1,1)) * np.ones(shape)\n",
    "#\n",
    "#\n",
    "# # If only a a subset is augmented, choose a random subset of X_train\n",
    "# if n_augm:\n",
    "#     np.random.seed(random_state_train)\n",
    "#     X_augm = offset + X_train[np.random.choice(X_train.shape[0], n_augm, replace=False)]\n",
    "#     np.random.seed(random_state_train)\n",
    "#     y_augm = offset + y_train[np.random.choice(y_train.shape[0], n_augm, replace=False)]\n",
    "# else:\n",
    "#     X_augm = offset + X_train \n",
    "#     y_augm = offset + y_train\n",
    "#\n",
    "# X_train = np.concatenate((X_train, X_augm))\n",
    "# y_train = np.concatenate((y_train, y_augm))\n",
    "\n",
    "# Convert all NaNs to zero\n",
    "X_train = np.nan_to_num(X_train)\n",
    "y_train = np.nan_to_num(y_train)\n",
    "\n",
    "X_valid = np.nan_to_num(X_valid)\n",
    "y_valid = np.nan_to_num(y_valid)\n",
    "\n",
    "# Shuffle again, if necessary\n",
    "# np.random.seed(random_state_train)\n",
    "# np.random.shuffle(X_train)\n",
    "# np.random.seed(random_state_train)\n",
    "# np.random.shuffle(y_train)\n",
    "\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "# print(\"X_augm shape: \", X_augm.shape)\n",
    "print(\"X_valid shape: \", X_valid.shape)\n",
    "\n",
    "# del X_augm, y_augm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83feecd9-5f58-482f-89ef-300ffc25ec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Downsampled Skip-Conncetion - Multi Scale model by Fukami 2019\n",
    "print(\"Initializing Model\")\n",
    "# Original Model\n",
    "input_img = Input(shape=(grid[0], grid[1], dim))\n",
    "\n",
    "#Downsampled skip-connection model\n",
    "down_1 = MaxPooling2D((8,8), padding='same')(input_img)\n",
    "x1 = Conv2D(32, (3,3), activation='relu', padding='same')(down_1)\n",
    "x1 = Conv2D(32, (3,3), activation='relu', padding='same')(x1)\n",
    "x1 = UpSampling2D((2,2))(x1)\n",
    "\n",
    "down_2 = MaxPooling2D((4,4), padding='same')(input_img)\n",
    "x2 = Concatenate()([x1, down_2])\n",
    "x2 = Conv2D(32, (3,3), activation='relu', padding='same')(x2)\n",
    "x2 = Conv2D(32, (3,3), activation='relu', padding='same')(x2)\n",
    "x2 = UpSampling2D((2,2))(x2)\n",
    "\n",
    "down_3 = MaxPooling2D((2,2), padding='same')(input_img)\n",
    "x3 = Concatenate()([x2, down_3])\n",
    "x3 = Conv2D(32, (3,3), activation='relu', padding='same')(x3)\n",
    "x3 = Conv2D(32, (3,3), activation='relu', padding='same')(x3)\n",
    "x3 = UpSampling2D((2,2))(x3)\n",
    "\n",
    "x4 = Concatenate()([x3, input_img])\n",
    "x4 = Conv2D(32, (3,3), activation='relu', padding='same')(x4)\n",
    "x4 = Conv2D(32, (3,3), activation='relu', padding='same')(x4)\n",
    "\n",
    "#Multi-scale model (Du et al., 2018)\n",
    "layer_1 = Conv2D(16, (5,5), activation='relu', padding='same')(input_img)\n",
    "x1m = Conv2D(8, (5,5), activation='relu', padding='same')(layer_1)\n",
    "x1m = Conv2D(8, (5,5), activation='relu', padding='same')(x1m)\n",
    "\n",
    "layer_2 = Conv2D(16, (9,9), activation='relu', padding='same')(input_img)\n",
    "x2m = Conv2D(8, (9,9), activation='relu', padding='same')(layer_2)\n",
    "x2m = Conv2D(8, (9,9), activation='relu', padding='same')(x2m)\n",
    "\n",
    "layer_3 = Conv2D(16, (13,13), activation='relu', padding='same')(input_img)\n",
    "x3m = Conv2D(8, (13,13), activation='relu', padding='same')(layer_3)\n",
    "x3m = Conv2D(8, (13,13), activation='relu', padding='same')(x3m)\n",
    "\n",
    "x_add = Concatenate()([x1m, x2m, x3m, input_img])\n",
    "x4m = Conv2D(8, (7,7), activation='relu', padding='same')(x_add)\n",
    "x4m = Conv2D(3, (5,5), activation='relu', padding='same')(x4m)\n",
    "\n",
    "x_final = Concatenate()([x4, x4m])\n",
    "x_final = Conv2D(dim, (3,3), padding='same')(x_final)\n",
    "autoencoder = Model(input_img, x_final)\n",
    "# Using MSE as performance indicator\n",
    "autoencoder.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d837178c-aa8c-4a85-a62a-828690f90d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file output names\n",
    "fdir = \"Models/\"\n",
    "if not os.path.isdir(fdir):\n",
    "    os.makedirs(fdir)\n",
    "\n",
    "# Prepare the file names for parameter summary, model and training history\n",
    "\n",
    "#fmodel = \"Model_Inp_{}_{}_Kernel_{}_{}_{{epoch:02d}}.hdf5\".format(var, pooling, kernel, today)\n",
    "fmodel = \"Model_Inp_{}_{}_Kernel_{}_{}.hdf5\".format(var, pooling, kernel, today)\n",
    "fmodel = os.path.join(fdir, fmodel)\n",
    "\n",
    "fhist = \"History_Inp_{}_{}_Kernel_{}_{}.csv\".format(var, pooling, kernel, today)\n",
    "fhist = os.path.join(fdir, fhist)\n",
    "\n",
    "fsum = \"Summary_Inp_{}_{}_Kernel_{}_{}.txt\".format(var, pooling, kernel, today)\n",
    "fsum = os.path.join(fdir, fsum)\n",
    "\n",
    "# Check if filename already exists and if it does uniquify it\n",
    "if os.path.isfile(fmodel):\n",
    "    fmodel, fhist, fsum = uniquify([fmodel, fhist, fsum])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c727e349-b5e7-4e84-bffb-00c5898a720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Callbacks\n",
    "model_cb = ModelCheckpoint(fmodel, monitor='val_loss',\n",
    "                           save_best_only=True, verbose=1)\n",
    "early_cb = EarlyStopping(monitor='val_loss', patience=patience, verbose=1)\n",
    "cb = [model_cb, early_cb]\n",
    "\n",
    "# Train Neural Network and measure how long training takes\n",
    "t0 = time()\n",
    "history = autoencoder.fit(X_train, y_train, epochs=5000, batch_size=batch_size,\n",
    "                          verbose=1, callbacks=cb, shuffle=shuffle,\n",
    "                          validation_data=(X_valid, y_valid))\n",
    "t0 = time()-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95272f4-b561-436a-970c-7c062bc96298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before loading and evaluating test data free memory\n",
    "del X_train, y_train, X_valid, y_valid\n",
    "\n",
    "# Evaluate with a year of data\n",
    "X_test = np.zeros((len(serial_test), grid[0], grid[1], dim))\n",
    "y_test = np.zeros((len(serial_test), grid[0],grid[1], dim))\n",
    "# Iterate through all the datasets and save them in their arrays\n",
    "for i, s in enumerate(serial_test):\n",
    "\n",
    "    # Load reference data\n",
    "    df = pd.read_csv(fname_HR.format(s), header=None, delim_whitespace=False)\n",
    "    y_test[i,:,:,0] = df.values\n",
    "\n",
    "    # Load input data\n",
    "    df = pd.read_csv(fname_LR.format(s), header=None, delim_whitespace=False)\n",
    "    X_test[i,:,:,0] = df.values\n",
    "\n",
    "X_test = np.nan_to_num(X_test)\n",
    "y_test = np.nan_to_num(y_test)\n",
    "\n",
    "ev = autoencoder.evaluate(X_test, y_test)\n",
    "\n",
    "# Save Model History\n",
    "df_results = pd.DataFrame(history.history)\n",
    "df_results['epoch'] = history.epoch\n",
    "df_results.to_csv(fhist, index=False)\n",
    "\n",
    "min_val_loss = min(history.history[\"val_loss\"])\n",
    "# Save Model Summary\n",
    "with open(fsum, \"w\") as f:\n",
    "    summary = summary.format(today, kernel, pooling, var, train_size, shuffle, patience,\n",
    "                             batch_size, t0, random_state_train, random_state_test,\n",
    "                             sample_start, sample_end, min_val_loss, ev, n_augm)\n",
    "    f.write(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
